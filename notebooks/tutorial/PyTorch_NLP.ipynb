{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorchの演算の扱い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1002bcd50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.9718  1.7070 -0.4305 -2.2820  0.5237\n",
      " 0.0004 -1.2039  3.5283  0.4434  0.5848\n",
      " 0.8407  0.5510  0.3863  0.9124 -0.8410\n",
      " 1.2282 -1.8661  1.4146 -1.8781 -0.4674\n",
      "-0.7576  0.4215 -0.4827 -1.1198  0.3056\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      " 1.0386  0.5206 -0.5006 -1.9441 -0.9596  0.5489 -0.9901 -0.3826\n",
      " 1.2182  0.2117 -1.0613  1.5037  1.8267  0.5561  1.6445  0.4973\n",
      "[torch.FloatTensor of size 2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#行を連結\n",
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "#列を連結\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -1.5067  1.7661 -0.3569 -0.1713\n",
      "  0.4068 -0.4284 -1.1299  1.4274\n",
      " -1.4027  1.4825 -1.1559  1.6190\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.9581  0.7747  0.1940  0.1687\n",
      "  0.3061  1.0743 -1.0327  1.0930\n",
      "  0.7769 -1.3128  0.7099  0.9944\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-1.5067  1.7661 -0.3569 -0.1713  0.4068 -0.4284 -1.1299  1.4274 -1.4027  1.4825\n",
      " 0.9581  0.7747  0.1940  0.1687  0.3061  1.0743 -1.0327  1.0930  0.7769 -1.3128\n",
      "\n",
      "Columns 10 to 11 \n",
      "-1.1559  1.6190\n",
      " 0.7099  0.9944\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-1.5067  1.7661 -0.3569 -0.1713  0.4068 -0.4284 -1.1299  1.4274 -1.4027  1.4825\n",
      " 0.9581  0.7747  0.1940  0.1687  0.3061  1.0743 -1.0327  1.0930  0.7769 -1.3128\n",
      "\n",
      "Columns 10 to 11 \n",
      "-1.1559  1.6190\n",
      " 0.7099  0.9944\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#.view：reshape関数\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "\n",
    "#2*12にreshape\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "\n",
    "#上記と同じ。最終層を一つ減らす。サイズは推論される\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#作成したデータ。文章を単語ごとに分けて格納する\n",
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 3, 'No': 9, 'buena': 14, 'it': 7, 'at': 22, 'sea': 12, 'cafeteria': 5, 'Yo': 23, 'la': 4, 'to': 8, 'creo': 10, 'is': 16, 'a': 18, 'good': 19, 'get': 20, 'idea': 15, 'que': 11, 'not': 17, 'me': 0, 'on': 25, 'gusta': 1, 'lost': 21, 'Give': 6, 'una': 13, 'si': 24, 'comer': 2}\n",
      "\n",
      "Num of vocab size: 26\n"
     ]
    }
   ],
   "source": [
    "#出てくる単語にidを割り振る\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            #長さは1ずつ伸びるのでこれをidとして利用\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "print\n",
    "print (('Num of vocab size: ') + str(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Wordsでのロジスティック回帰をpytorchで実装する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ output = Log\\_Softmax(Logistic(input)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Logistic(x) = \\frac{1}{1+e^{-(ax+b)}} $$ <br>\n",
    "$$ Log\\_Softmax(x_i) = log(\\frac{e^x_i}{\\sum_j(e^{x_j})}) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BoW識別器の実装\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        #必ず継承の必要がある\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        #input: vocabraryの種類の数\n",
    "        #output: labelの数\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec))\n",
    "\n",
    "#文章をBoW表現のvectorに変換\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    #vocabraryの種類の長さのvector\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    \n",
    "    #文章中の単語をそれぞれのvectorに+1ずつ行う\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "#labelをTensor型に変換\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n",
    "\n",
    "#モデル定義\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.1560  0.0131 -0.0337  0.1765  0.0763 -0.0027 -0.0337  0.0159 -0.1765  0.1041\n",
      " 0.1206 -0.0480 -0.0401  0.0151 -0.1313  0.0597  0.1677 -0.0544 -0.0597  0.0279\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0141 -0.1783  0.0642 -0.1412  0.0058  0.1147  0.1744 -0.1844  0.0339  0.1503\n",
      " 0.0984  0.0541  0.0886 -0.1466  0.1503  0.0746  0.0485  0.0580  0.0984 -0.0573\n",
      "\n",
      "Columns 20 to 25 \n",
      " 0.1582  0.0160 -0.1422 -0.0204 -0.1415  0.1538\n",
      "-0.0593  0.1032 -0.0902 -0.0563  0.1553  0.0992\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      "-0.0282\n",
      " 0.1496\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#モデルのパラメータ数の確認\n",
    "#単語の種類が26, English, Spanishでラベルが2種類\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ\n",
      "(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH')\n",
      "入力用ベクトル\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    1     1     1     1     1     1     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x26]\n",
      "\n",
      "スペイン語, 英語の対数確率\n",
      "Variable containing:\n",
      "-0.7341 -0.6538\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#データ・入力ベクトル・対数確率の確認\n",
    "sample = data[0]\n",
    "print (\"データ\")\n",
    "print (sample)\n",
    "\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "print (\"入力用ベクトル\")\n",
    "print (bow_vector)\n",
    "\n",
    "log_probs = model(Variable(bow_vector))\n",
    "print (\"スペイン語, 英語の対数確率\")\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.1405 -0.3852\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.8490 -0.5583\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "What is the parameter of the layer of spanish word 'creo'\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.4080\n",
      "  9.8377\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "SPANISH\n",
      "probability: Variable containing:\n",
      "-0.2095 -1.6658\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "ENGLISH\n",
      "probability: Variable containing:\n",
      "-2.6878 -0.0705\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Check the spanish word 'creo' goes up\n",
      "Variable containing:\n",
      " 0.4846\n",
      "-0.3721\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n",
    "\n",
    "# forwardのみ\n",
    "for instance, label in test_data:\n",
    "    bow_vec = Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(\"What is the parameter of the layer of spanish word 'creo'\")\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
    "\n",
    "#Loss関数の計算\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "#optimizerの設定\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#普通は5~30エポック程度でOK\n",
    "#今度はback propagation有り\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        \n",
    "        # 前の勾配情報をリセットする\n",
    "        model.zero_grad()\n",
    "\n",
    "        #Pytorchのモデルに入れられるように変換\n",
    "        bow_vec = Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = Variable(make_target(label, label_to_ix))\n",
    "\n",
    "        # forward step\n",
    "        log_probs = model(bow_vec)\n",
    "\n",
    "        #Lossの計算とbackpropagation\n",
    "        #最適化\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "for instance, label in test_data:\n",
    "    bow_vec = Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print (label)\n",
    "    print((\"probability: \") + str(log_probs))\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "print(\"Check the spanish word 'creo' goes up\")\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たしかに文章1はスペイン語の確率大・文章2は英語の確率大になるよう学習されている<br>\n",
    "\"creo\"というワードもスペイン語の可能性が高いようにレイヤー学習が進んでいる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words形式は単語にそれぞれidを振るが、\n",
    "<ul>\n",
    "    <li>数多くなったら計算大変\n",
    "    <li>単語の関係性が全くわからない(意味の近さ・遠さなど)\n",
    "</ul>\n",
    "といった問題点がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.2694  0.1495 -0.0336 -0.6076 -1.0048\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#各単語をインデックス化\n",
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "\n",
    "#合計二つの単語を5次元ベクトルに変換\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "\n",
    "#Tensorに型変換\n",
    "lookup_tensor = torch.LongTensor([word_to_ix[\"hello\"]])\n",
    "print(lookup_tensor)\n",
    "\n",
    "#\"hello\"をベクトル化(ここでは最適化されていない)\n",
    "hello_embed = embeds(Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gramを実装する。N-gramは単語$wi$前のN単語から次の単語を推測する<br>\n",
    "$P(w_i|w_{i−1},w_{i−2},…,w_{i−n+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#前の前後関係：前2単語のみ見る\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "#埋め込んだ次元\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# 文章サンプル\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "# tri-gramの例\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "\n",
    "#中身を確認\n",
    "#when fortyときたら次はwintersを予測する\n",
    "# print the first 3, just so you can see what they look like\n",
    "print(trigrams[:3])\n",
    "\n",
    "#登場する単語にインデックスを割り振る\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'forty']\n",
      "[41, 25]\n",
      "vocabrary size: 97\n",
      "vocabrary size: 10\n",
      "Variable containing:\n",
      "-0.8429 -0.2405 -0.5897  1.4151  0.3399  0.0112  1.1586 -0.2975 -1.7608  1.7539\n",
      "-0.1815 -0.6644  0.4104  0.8742 -0.1276  0.6828 -1.9091  1.4970 -1.2883 -0.0708\n",
      "[torch.FloatTensor of size 2x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context, target = trigrams[0]\n",
    "context_idxs = [word_to_ix[w] for w in context]\n",
    "\n",
    "# 変換元の単語\n",
    "print context\n",
    "\n",
    "# 単語のid\n",
    "print context_idxs\n",
    "\n",
    "# Tensor変換\n",
    "context_var = Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "# 単語id -> vector変換\n",
    "# 2単語 × vector変換 ： (embedding層　× 2) のサイズ\n",
    "embeddings = nn.Embedding(len(vocab), EMBEDDING_DIM)\n",
    "\n",
    "print \"vocabrary size: \" + str(len(vocab))\n",
    "print \"vocabrary size: \" + str(EMBEDDING_DIM)\n",
    "print embeddings(context_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#N-Gramモデルの生成\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        \n",
    "        # 合計の単語の種類を潜在次元に落とし込む\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ロスの確認\n",
    "losses = []\n",
    "\n",
    "#ロス関数の定義\n",
    "#The negative log likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "#モデル定義\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "\n",
    "#optimizerの定義\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NGramLanguageModeler.parameters of NGramLanguageModeler (\n",
      "  (embeddings): Embedding(97, 10)\n",
      "  (linear1): Linear (20 -> 128)\n",
      "  (linear2): Linear (128 -> 97)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 519.7821\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 517.4017\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 515.0360\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 512.6841\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 510.3459\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 508.0204\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 505.7052\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 503.4016\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 501.1090\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 498.8246\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#トレーニング\n",
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    \n",
    "    # 各単語の予測\n",
    "    # 前のN単語を見て次の単語を予測する\n",
    "    for context, target in trigrams:\n",
    "        \n",
    "        # 文章を読み込める形式に変える\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # 勾配初期化\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # モデル\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        #ロス関数の計算\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        #誤差逆伝播法\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Word Embeddings: Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBoW： 前後の数ワードから中の単語を予測する<br>\n",
    "embeddingsの初期化に用いられる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語$w_i$に対して、前後N個のキーワード$w_i−1,…,w_{i−N}$、$w_{i+1},…,w_{i+N}$が与えられた時に、<br>\n",
    "$q_w$をembeddingしたwordとすれば、以下の確率を最小化するものを考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$−log⁡p(w_i|C)=−log⁡Softmax(A(\\sum_{w∈C}q_w)+b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this model in Pytorch by filling in the class below. Some tips:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about which parameters you need to define.<br>\n",
    "Make sure you know what shape each operation expects. Use .view() if you need to reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "# 前後2単語ずつを推測に用いる\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "\n",
    "#埋め込んだ次元\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "# 原文\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# インデックス化\n",
    "word_to_ix = {word: i for i, word in enumerate(raw_text)}\n",
    "\n",
    "# 前後2単語ずつと予測単語の組み合わせを作成する\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  0\n",
       " 13\n",
       " 47\n",
       "  4\n",
       "[torch.LongTensor of size 4]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CBOWのクラス定義\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(2 * context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix)  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ロスの確認\n",
    "losses = []\n",
    "\n",
    "#ロス関数の定義\n",
    "#The negative log likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "#モデル定義\n",
    "model = CBOW(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "\n",
    "#optimizerの定義\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CBOW.parameters of CBOW (\n",
       "  (embeddings): Embedding(97, 10)\n",
       "  (linear1): Linear (40 -> 128)\n",
       "  (linear2): Linear (128 -> 97)\n",
       ")>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 266.7071\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 264.9733\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 263.2492\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 261.5334\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 259.8259\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 258.1265\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 256.4332\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 254.7435\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 253.0574\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 251.3734\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#トレーニング\n",
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    \n",
    "    # 各単語の予測\n",
    "    # 前のN単語を見て次の単語を予測する\n",
    "    for context, target in data:\n",
    "        \n",
    "        # 文章を読み込める形式に変える\n",
    "        context_idxs = [word_to_ix[w] for w in context]\n",
    "        context_var = Variable(torch.LongTensor(context_idxs))\n",
    "\n",
    "        # 勾配初期化\n",
    "        model.zero_grad()\n",
    "\n",
    "        # モデル\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        #ロス関数の計算\n",
    "        loss = loss_function(log_probs, Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        #誤差逆伝播法\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Models and Long-Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMの挙動をチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       " -1.5859 -1.4814  0.4191\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       " -0.9734  0.4680  1.6193\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       " -0.8317  1.1417  0.2224\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       "  1.3163  1.7850  1.3064\n",
       " [torch.FloatTensor of size 1x3], Variable containing:\n",
       "  0.3383 -0.6922  0.9433\n",
       " [torch.FloatTensor of size 1x3]]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM層の定義\n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [Variable(torch.randn((1, 3)))\n",
    "    for _ in range(5)]  # make a sequence of length 5\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " (0 ,.,.) = \n",
       "   0.7190  0.9216 -1.0710\n",
       " [torch.FloatTensor of size 1x1x3], Variable containing:\n",
       " (0 ,.,.) = \n",
       "  -0.2065  1.0174 -0.3371\n",
       " [torch.FloatTensor of size 1x1x3])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#隠れ層の初期化\n",
    "# initialize the hidden state.\n",
    "hidden = (Variable(torch.randn(1, 1, 3)),\n",
    "          Variable(torch.randn((1, 1, 3))))\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.5859 -1.4814  0.4191\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.5859 -1.4814  0.4191\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1x3を3次元にするために1x1x3(バッチサイズ1に等しい)に変換\n",
    "print(inputs[0])\n",
    "print(inputs[0].view(1, 1, -1))\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.5859 -1.4814  0.4191\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.5859 -1.4814  0.4191\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1221  0.2631  0.1254\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1221  0.2631  0.1254\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1675  0.5136  0.3686\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#inputs内の各値に対してLSTMを通す\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Variable containing:\n",
      "-1.5859 -1.4814  0.4191\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      "-0.9734  0.4680  1.6193\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      "-0.8317  1.1417  0.2224\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      " 1.3163  1.7850  1.3064\n",
      "[torch.FloatTensor of size 1x3]\n",
      ", Variable containing:\n",
      " 0.3383 -0.6922  0.9433\n",
      "[torch.FloatTensor of size 1x3]\n",
      "]\n",
      "Variable containing:\n",
      "-1.5859 -1.4814  0.4191\n",
      "-0.9734  0.4680  1.6193\n",
      "-0.8317  1.1417  0.2224\n",
      " 1.3163  1.7850  1.3064\n",
      " 0.3383 -0.6922  0.9433\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -1.5859 -1.4814  0.4191\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.9734  0.4680  1.6193\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.8317  1.1417  0.2224\n",
      "\n",
      "(3 ,.,.) = \n",
      "  1.3163  1.7850  1.3064\n",
      "\n",
      "(4 ,.,.) = \n",
      "  0.3383 -0.6922  0.9433\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for文を使わないための変換\n",
    "#元のinputs\n",
    "print(inputs)\n",
    "\n",
    "#1x3が5個あるやつをくっつける\n",
    "print(torch.cat(inputs))\n",
    "\n",
    "#5x1x3に変換\n",
    "print(torch.cat(inputs).view(len(inputs), 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.5690  0.2984 -0.2307\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.4488  0.2394  0.0703\n",
      "\n",
      "(2 ,.,.) = \n",
      " -0.3698  0.0887  0.3511\n",
      "\n",
      "(3 ,.,.) = \n",
      " -0.4498  0.3913  0.2789\n",
      "\n",
      "(4 ,.,.) = \n",
      " -0.4714  0.1113  0.4174\n",
      "[torch.FloatTensor of size 5x1x3]\n",
      "\n",
      "(Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.4714  0.1113  0.4174\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.6453  0.1624  0.5910\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#for文なしでまとめて実行\n",
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropogate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (Variable(torch.randn(1, 1, 3)), Variable(\n",
    "    torch.randn((1, 1, 3))))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: An LSTM for Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Everybody': 5, 'ate': 2, 'apple': 4, 'that': 7, 'read': 6, 'dog': 1, 'book': 8, 'the': 3, 'The': 0}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n",
    "#トレーニングデータ\n",
    "#単語と品詞の対応\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "#単語のインデックス\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "#品詞のインデックス\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "#通常は32か64次元\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        # 隠れ層の次元\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding層\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTMの定義\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # 潜在空間\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        #LSTMの隠れ層の定義\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    #隠れ層の定義\n",
    "    def init_hidden(self):\n",
    "        # 隠れ層は(num_layers, minibatch_size, hidden_dim)のように定義する必要がある\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    \n",
    "    #順伝播\n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        #embedding\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        #LSTM\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        \n",
    "        #Linear\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        \n",
    "        #Log_Softmax\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#モデル定義諸々\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.9500 -1.1105 -1.2593\n",
      "-0.9220 -1.1113 -1.2978\n",
      "-1.0782 -1.0665 -1.1534\n",
      "-0.9738 -1.0909 -1.2502\n",
      "-1.0779 -1.0665 -1.1537\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LSTMに通してみる\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#トレーニング\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    \n",
    "    #文章ごとの処理\n",
    "    for sentence, tags in training_data:\n",
    "        \n",
    "        #勾配初期化\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        #隠れ層(LSTMの記憶部分)の状態を初期化\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        #文章変換\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Variables of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1564 -1.9627 -5.4380\n",
      "-4.4316 -0.0133 -6.6546\n",
      "-5.4240 -4.4229 -0.0165\n",
      "-0.0224 -4.0915 -5.2090\n",
      "-5.2533 -0.0105 -5.2639\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what the scores are after training\n",
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "#スコアチェック\n",
    "tag_scores = model(inputs)\n",
    "# The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "#  for word i. The predicted tag is the maximum scoring tag.\n",
    "# Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "# since 0 is index of the maximum value of row 1,\n",
    "# 1 is the index of maximum value of row 2, etc.\n",
    "# Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw: ['The', 'dog', 'ate', 'the', 'apple']\n",
      "target: ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "predict: ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "#予測値についての確認\n",
    "print \"raw: \"+ str(training_data[0][0])\n",
    "print \"target: \"+ str(training_data[0][1])\n",
    "print \"predict: \" + str([tag_to_ix.keys()[i] for i in torch.max(tag_scores,1)[1].data.numpy().T[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Augmenting the LSTM part-of-speech tagger with character-level features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMの品詞予測に文字レベルの特徴量を追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, each word had an embedding, which served as the inputs to our sequence model. Let’s augment the word embeddings with a representation derived from the characters of the word. We expect that this should help significantly, since character-level information like affixes have a large bearing on part-of-speech. For example, words with the affix -ly are almost always tagged as adverbs in English.\n",
    "\n",
    "Do do this, let cwcw be the character-level representation of word ww. Let xwxw be the word embedding as before. Then the input to our sequence model is the concatenation of xwxw and cwcw. So if xwxw has dimension 5, and cwcw dimension 3, then our LSTM should accept an input of dimension 8.\n",
    "\n",
    "To get the character level representation, do an LSTM over the characters of a word, and let cwcw be the final hidden state of this LSTM. Hints:\n",
    "\n",
    "There are going to be two LSTM’s in your new model. The original one that outputs POS tag scores, and the new one that outputs a character-level representation of each word.\n",
    "To do a sequence model over characters, you will have to embed characters. The character embeddings will be the input to the character LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, each word had an embedding, which served as the inputs to our sequence model. Let’s augment the word embeddings with a representation derived from the characters of the word. We expect that this should help significantly, since character-level information like affixes have a large bearing on part-of-speech. For example, words with the affix -ly are almost always tagged as adverbs in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Making Dynamic Decisions and the Bi-LSTM CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic versus Static Deep Learning Toolkits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KerasやTheano：Static<br>\n",
    "Pytorch：Dynamic<br>\n",
    "なので、毎ループの度にグラフが計算される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この違いを考えるために、木構造のグラフを考える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>ボトムアップの木を作成する\n",
    "<li>根に単語や文のノードをつける\n",
    "<li>NNやEmbeddingを用いて、この木構造を定める\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このような場合、KerasのようなStaticなネットワークは難しい<br>\n",
    "またTensorFlowなどと比較するとより通常のpythonに近い(class定義でモデルを指定するなど)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM Conditional Random Field Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "固有名詞の抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xを入力の単語のシーケンス、yを単語についての品詞のシーケンスだとすると、これらの事象の確率は以下のように表されるm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\log \\psi_i(x,y)$をLogポテンシャルとすると、上記のスコアは以下のように表される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-LSTM CRFにおいて、二つのポテンシャル(emissionとtransition)を定義する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "index iの単語のBiLSTMのTimestep i における隠れ状態とする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the Bi-LSTM CRF, we define two kinds of potentials: emission and\n",
    "transition. The emission potential for the word at index $i$ comes\n",
    "from the hidden state of the Bi-LSTM at timestep $i$. The\n",
    "transition scores are stored in a $|T|x|T|$ matrix\n",
    "$\\textbf{P}$, where $T$ is the tag set. In my\n",
    "implementation, $\\textbf{P}_{j,k}$ is the score of transitioning\n",
    "to tag $j$ from tag $k$. So:\n",
    "\n",
    "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}\n",
    "\n",
    "\\begin{align}= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ヘルパー関数\n",
    "def to_scalar(var):\n",
    "    # returns a python float\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#モデル定義\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # LSTMをtagに変換するレイヤー\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        \n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    #隠れ層の初期化\n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.randn(2, 1, self.hidden_dim)),\n",
    "                Variable(torch.randn(2, 1, self.hidden_dim)))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = Variable(init_alphas)\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward variables at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = Variable(init_vvars)\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id])\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        self.hidden = self.init_hidden()\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        self.hidden = self.init_hidden()\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 15.9463\n",
      "[torch.FloatTensor of size 1]\n",
      ", [2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2])\n",
      "(Variable containing:\n",
      " 39.0223\n",
      "[torch.FloatTensor of size 1]\n",
      ", [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [(\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ")]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "# Check predictions before training\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "precheck_tags = torch.LongTensor([tag_to_ix[t] for t in training_data[0][1]])\n",
    "print(model(precheck_sent))\n",
    "\n",
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
    "for epoch in range(\n",
    "        300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Variables of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.LongTensor([tag_to_ix[t] for t in tags])\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        neg_log_likelihood.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Check predictions after training\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "print(model(precheck_sent))\n",
    "# We got it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
